{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9261ef8-5120-482e-8b06-d2ed8fbe29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENBLAS_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env OPENBLAS_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336f344c-1add-4060-a06e-bab5282273f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lc2382/.conda/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "# HF imports\n",
    "import diffusers\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import datasets\n",
    "\n",
    "# custom imports\n",
    "from training import train_loop\n",
    "from eval import evaluate_generation, evaluate_sample_many, evaluate_fake_PIRADS_images, evaluate, SegGuidedDDIMPipeline\n",
    "from utils import make_grid, save_nifti, load_config, flatten_config, parse_3d_volumes, split_dset_by_patient, get_patient_splits\n",
    "\n",
    "import yaml\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import monai\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, ScaleIntensityRanged, NormalizeIntensityd, ToTensord, Orientationd, CenterSpatialCropd, Orientationd, ScaleIntensityRangePercentilesd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b22279-26ec-44cd-b63d-5936df85ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "    \"\"\"Load a YAML configuration file and return a nested dictionary.\"\"\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def flatten_config(config):\n",
    "    \"\"\"\n",
    "    Merge nested configuration dictionaries into a single flat dictionary.\n",
    "    \n",
    "    For each top-level section (e.g., model_args, data_args, etc.), the inner\n",
    "    dictionary is merged into a single dictionary. If there are duplicate keys,\n",
    "    later sections will overwrite the earlier ones.\n",
    "    \"\"\"\n",
    "    flat_config = {}\n",
    "    for section, sub_config in config.items():\n",
    "        if isinstance(sub_config, dict):\n",
    "            flat_config.update(sub_config)\n",
    "        else:\n",
    "            flat_config[section] = sub_config\n",
    "    return flat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fccaaac-6a8e-4f86-affb-97124a42e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mode': 'train', 'model_type': 'DDIM', 'resume_epoch': None, 'img_size': 128, 'num_img_channels': 1, 'dataset': 'fastMRI_NYU', 'label_csv_dir': '/home/lc2382/project/segmentation-guided-diffusion/data/labels/t2_slice_level_labels_reversed.csv', 'img_dir': '/home/lc2382/project/fastMRI_NYU/nifti', 'seg_dir': '/home/lc2382/project/segmentation-guided-diffusion/data/CG+PZ_seg', 'segmentation_guided': True, 'neighboring_images_guided': True, 'segmentation_channel_mode': 'single', 'num_segmentation_classes': 3, 'train_batch_size': 32, 'eval_batch_size': 16, 'num_epochs': 400, 'gradient_accumulation_steps': 1, 'learning_rate': 1e-05, 'lr_warmup_steps': 500, 'save_image_epochs': 40, 'save_model_epochs': 40, 'mixed_precision': 'fp16', 'class_conditional': True, 'cfg_training': True, 'num_class_embeds': 5, 'output_dir': None, 'save_forward_process': True, 'use_squaredcos_cap_v2_scheduler': True, 'use_ablated_segmentations': False, 'eval_noshuffle_dataloader': False, 'seed': 10383, 'cfg_eval': True, 'fake_labels': True, 'eval_mask_removal': False, 'eval_blank_mask': False, 'eval_sample_size': 1000, 'cfg_p_uncond': 0.2, 'cfg_weight': 1.0, 'trans_noise_level': 0.5, 'use_cfg_for_eval_conditioning': True, 'cfg_maskguidance_condmodel_only': True, 'push_to_hub': False, 'hub_private_repo': False, 'overwrite_output_dir': True, 'debug_save_image': True}\n"
     ]
    }
   ],
   "source": [
    "config = flatten_config(load_config('/home/lc2382/project/segmentation-guided-diffusion/config/config_seg_guided_CG+PZ_CFG_ClassCond_CosScheduler_400epochs_cfgweight1.0.yaml'))\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4afe54c-9016-4e04-90a1-30da5487ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] splits seed10383\n",
      "[main] train ids\n",
      " ['094', '042', '161', '087', '279', '108', '082', '123', '240', '165', '097', '118', '145', '047', '153', '196', '053', '026', '294', '290', '154', '036', '023', '210', '050', '091', '090', '259', '194', '233', '296', '078', '105', '009', '051', '214', '010', '192', '103', '238', '098', '020', '190', '136', '121', '189', '171', '077', '032', '049', '001', '160', '046', '104', '207', '178', '297', '005', '228', '293', '086', '022', '183', '175', '025', '006', '135', '100', '312', '137', '250', '088', '239', '199', '162', '182', '107', '251', '280', '260', '133', '035', '152', '170', '289', '221', '101', '168', '166', '111', '285', '282', '015', '045', '127', '138', '062', '116', '126', '211', '004', '065', '054', '203', '298', '269', '306', '040', '141', '041', '106', '072', '209', '212', '261', '179', '027', '109', '148', '277', '267', '262', '245', '149', '191', '284', '080', '061', '268', '029', '151', '066', '304', '193', '144', '305', '033', '018', '263', '037', '243', '164', '079', '028', '064', '220', '204', '120', '303', '230', '095', '092', '249', '075', '188', '216', '266', '044', '225', '208', '155', '074', '071', '248', '169', '016', '019', '288', '185', '084', '069', '173', '114', '219', '247', '102', '158', '130', '099', '122', '146', '271', '031', '202', '177', '063', '292', '132', '234', '147', '232', '257', '113', '229', '124', '060', '039', '057', '270', '176', '307', '254', '112', '131', '156', '236', '205', '012', '167', '187', '115', '227', '308', '038', '273', '081', '083']\n",
      "[main] val ids\n",
      " ['226', '003', '089', '043', '286', '222', '244', '302', '172', '217', '224', '134', '076', '272', '200', '125', '252', '201', '093', '180', '242', '301', '052', '235', '163', '017', '281', '140', '300', '291', '309', '253', '287', '007', '246', '110', '264', '013', '119', '070', '258', '014', '048', '255', '215', '068', '096']\n",
      "[main] test ids\n",
      " ['241', '276', '311', '142', '034', '159', '128', '021', '073', '030', '067', '139', '278', '310', '157', '055', '002', '274', '237', '275', '184', '198', '195', '283', '299', '011', '058', '174', '295', '186', '181', '231', '213', '056', '129', '024', '143', '256', '206', '008', '117', '150', '218', '059', '197', '265', '085']\n",
      "\n",
      "Train: 217 patients\n",
      "val:   47 patients\n",
      "test:  47 patients\n",
      "\n",
      "[main] CONDITIONED ON CG+PZ_lambd0.4\n",
      "[main] dict_keys(['image', 'seg_CG+PZ_lambd0.4'])\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"/home/lc2382/project/fastMRI_NYU/nifti\"\n",
    "exclude = [\"223\"]\n",
    "train_ids, val_ids, test_ids = get_patient_splits(\n",
    "    datapath=config['img_dir'],\n",
    "    test_size=0.3,\n",
    "    val_size=0.5,\n",
    "    seed=10383,\n",
    "    exclude_ids=exclude\n",
    ")\n",
    "\n",
    "print(f\"[main] splits seed{config.get('seed', 42)}\")\n",
    "print(\"[main] train ids\\n\", train_ids)\n",
    "print(\"[main] val ids\\n\", val_ids)\n",
    "print(\"[main] test ids\\n\", test_ids)\n",
    "print()\n",
    "print(f\"Train: {len(train_ids)} patients\")\n",
    "print(f\"val:   {len(val_ids)} patients\")\n",
    "print(f\"test:  {len(test_ids)} patients\")\n",
    "print()\n",
    "\n",
    "# load data\n",
    "dset_dict = {}\n",
    "if config['img_dir'] is not None:\n",
    "    img_paths = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in sorted(os.walk(config['img_dir']))\n",
    "        for file in files if file.endswith(\"T2W.nii.gz\")\n",
    "    ]\n",
    "    dset_dict[\"image\"] = img_paths\n",
    "\n",
    "if config['segmentation_guided']:\n",
    "    seg_types = os.listdir(config['seg_dir'])\n",
    "    seg_paths = {\n",
    "        seg_type: [\n",
    "            os.path.join(root, file)\n",
    "            for root, _, files in sorted(os.walk(os.path.join(config['seg_dir'], seg_type)))\n",
    "            for file in files if file.endswith(\".nii.gz\")\n",
    "        ]\n",
    "        for seg_type in seg_types\n",
    "    }\n",
    "    for seg_type in seg_types:\n",
    "        print(\"[main] CONDITIONED ON\", seg_type)\n",
    "        seg_key = 'seg_' + seg_type\n",
    "        dset_dict.update({seg_key: seg_paths[seg_type]})\n",
    "\n",
    "print(f\"[main] {dset_dict.keys()}\")\n",
    "\n",
    "dset_dict_train = split_dset_by_patient(dset_dict, train_ids)\n",
    "dset_dict_val = split_dset_by_patient(dset_dict, val_ids)\n",
    "dset_dict_test = split_dset_by_patient(dset_dict, test_ids)\n",
    "\n",
    "slices_dset_list_train = parse_3d_volumes(dset_dict_train, seg_type, label_csv_file=config['label_csv_dir'])\n",
    "slices_dset_list_val = parse_3d_volumes(dset_dict_val, seg_type, label_csv_file=config['label_csv_dir'])\n",
    "slices_dset_list_test = parse_3d_volumes(dset_dict_test, seg_type, label_csv_file=config['label_csv_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f4467d-125a-4c75-aaf0-b5350a3b2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset is 3695\n",
      "Length of val_dataset is 797\n",
      "Length of test_dataset is 770\n"
     ]
    }
   ],
   "source": [
    "norm_key, tot_key = [], []\n",
    "if config['img_dir'] is not None:\n",
    "    norm_key.append('image')\n",
    "    tot_key.append('image')\n",
    "\n",
    "if config['segmentation_guided']:\n",
    "    tot_key.append(seg_key)\n",
    "\n",
    "if config['neighboring_images_guided']:\n",
    "    tot_key.append('clean_left')\n",
    "    tot_key.append('clean_right')\n",
    "    norm_key.append('clean_left')\n",
    "    norm_key.append('clean_right')\n",
    "\n",
    "train_transforms = Compose([\n",
    "\n",
    "    # Add a channel dimension to 'image' and 'segm'\n",
    "    EnsureChannelFirstd(keys=tot_key, channel_dim='no_channel'),\n",
    "\n",
    "    Orientationd(keys=tot_key, axcodes='LAS'),\n",
    "\n",
    "    # center spatial crop\n",
    "    CenterSpatialCropd(keys=tot_key, roi_size=(128, 128)),\n",
    "\n",
    "    # scale to [0, 1]\n",
    "    ScaleIntensityRangePercentilesd(\n",
    "        keys=norm_key,\n",
    "        lower=2.5, upper=97.5, \n",
    "        b_min=0.0, b_max=1.0, \n",
    "        clip=True, relative=False, \n",
    "        channel_wise=False\n",
    "    ),\n",
    "    \n",
    "    # normalize\n",
    "    NormalizeIntensityd(\n",
    "        keys=norm_key,\n",
    "        subtrahend=0.5,\n",
    "        divisor=0.5\n",
    "    ),\n",
    "    \n",
    "    # Convert 'image' and 'segm' to PyTorch tensors\n",
    "    ToTensord(keys=tot_key)\n",
    "])\n",
    "\n",
    "eval_transforms = Compose([\n",
    "\n",
    "    # Add a channel dimension to 'image' and 'segm'\n",
    "    EnsureChannelFirstd(keys=tot_key, channel_dim='no_channel'),\n",
    "\n",
    "    Orientationd(keys=tot_key, axcodes='LAS'),\n",
    "    \n",
    "    # center spatial crop\n",
    "    CenterSpatialCropd(keys=tot_key, roi_size=(config['img_size'], config['img_size'])),\n",
    "\n",
    "    # scale to [0, 1]\n",
    "    ScaleIntensityRangePercentilesd(\n",
    "        keys=norm_key,\n",
    "        lower=1.0, upper=99.0, \n",
    "        b_min=0.0, b_max=1.0, \n",
    "        clip=True, relative=False, \n",
    "        channel_wise=False\n",
    "    ),\n",
    "    \n",
    "    # normalize\n",
    "    NormalizeIntensityd(\n",
    "        keys=norm_key,\n",
    "        subtrahend=0.5,\n",
    "        divisor=0.5\n",
    "    ),\n",
    "\n",
    "    # Convert 'image' and 'segm' to PyTorch tensors\n",
    "    ToTensord(keys=tot_key)\n",
    "])\n",
    "\n",
    "train_dataset = monai.data.Dataset(slices_dset_list_train, transform=train_transforms)\n",
    "val_dataset = monai.data.Dataset(slices_dset_list_val, transform=eval_transforms)\n",
    "test_dataset = monai.data.Dataset(slices_dset_list_test, transform=eval_transforms)\n",
    "\n",
    "print(f\"Length of train_dataset is {len(train_dataset)}\")\n",
    "print(f\"Length of val_dataset is {len(val_dataset)}\")\n",
    "print(f\"Length of test_dataset is {len(test_dataset)}\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['train_batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['eval_batch_size'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31674e8b-b0a3-43c9-b679-b58061cff164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "axcodes ('LAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (320, 320), channels = 1,please make sure the input is in the channel-first format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patient_id', 'slice_idx', 'image', 'seg_CG+PZ_lambd0.4', 'clean_left', 'clean_right', 'class_label'])\n",
      "dict_keys(['patient_id', 'slice_idx', 'image', 'seg_CG+PZ_lambd0.4', 'clean_left', 'clean_right', 'class_label'])\n",
      "dict_keys(['patient_id', 'slice_idx', 'image', 'seg_CG+PZ_lambd0.4', 'clean_left', 'clean_right', 'class_label'])\n",
      "dict_keys(['patient_id', 'slice_idx', 'image', 'seg_CG+PZ_lambd0.4', 'clean_left', 'clean_right', 'class_label'])\n",
      "dict_keys(['patient_id', 'slice_idx', 'image', 'seg_CG+PZ_lambd0.4', 'clean_left', 'clean_right', 'class_label'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m labels   \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     15\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]           \u001b[38;5;66;03m# (B, C, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/data/dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Subset(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, indices\u001b[38;5;241m=\u001b[39mindex)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/data/dataset.py:98\u001b[0m, in \u001b[0;36mDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mFetch single data item from `self.data`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m data_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index]\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data_i\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars\u001b[38;5;241m.\u001b[39mdebug():\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars\u001b[38;5;241m.\u001b[39mdebug():\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/intensity/dictionary.py:1073\u001b[0m, in \u001b[0;36mScaleIntensityRangePercentilesd.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1071\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(data)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d):\n\u001b[0;32m-> 1073\u001b[0m     d[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/intensity/array.py:1412\u001b[0m, in \u001b[0;36mScaleIntensityRangePercentiles.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     img_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(img\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m img_t])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m     img_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_dst_type(img_t, dst\u001b[38;5;241m=\u001b[39mimg)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/intensity/array.py:1385\u001b[0m, in \u001b[0;36mScaleIntensityRangePercentiles._normalize\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_normalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: NdarrayOrTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NdarrayOrTensor:\n\u001b[0;32m-> 1385\u001b[0m     a_min: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1386\u001b[0m     a_max: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m percentile(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m     b_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_min\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/monai/transforms/utils_pytorch_numpy_unification.py:136\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(x, q, dim, keepdim, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     q \u001b[38;5;241m=\u001b[39m convert_to_dst_type(q_np \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100.0\u001b[39m, x)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 136\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# — assume you’ve already run your “train_dataloader = DataLoader(train_dataset, …)” code above —\n",
    "\n",
    "# 1) collect up to N samples\n",
    "max_samples = 1000\n",
    "features = []\n",
    "labels   = []\n",
    "count = 0\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    # print(batch.keys())\n",
    "    imgs = batch['image']           # (B, C, H, W)\n",
    "    segs = batch['seg_CG+PZ_lambd0.4']\n",
    "    lbs  = batch['class_label']           # (B,)\n",
    "    B = imgs.shape[0]\n",
    "\n",
    "    mask = (segs > 0).float()  # (B,1,H,W)\n",
    "\n",
    "    # apply mask so background is zeroed out\n",
    "    imgs_masked = imgs * mask   # (B,1,H,W)\n",
    "    \n",
    "    # flatten each image to a vector\n",
    "    flat = imgs_masked.view(B, -1).cpu().numpy()\n",
    "    features.append(flat)\n",
    "    labels.append(lbs.cpu().numpy())\n",
    "\n",
    "    count += B\n",
    "    if count >= max_samples:\n",
    "        break\n",
    "\n",
    "features = np.concatenate(features, axis=0)[:max_samples]\n",
    "labels   = np.concatenate(labels,   axis=0)[:max_samples]\n",
    "\n",
    "# 2) run t‑SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "emb  = tsne.fit_transform(features)   # (max_samples, 2)\n",
    "\n",
    "color_map = {\n",
    "    0: 'blue',\n",
    "    1: 'yellow',\n",
    "    2: 'red',\n",
    "    3: 'black',\n",
    "    4: 'green',\n",
    "}\n",
    "\n",
    "# 3) plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "for cls, color in color_map.items():\n",
    "    idx = labels == cls\n",
    "    plt.scatter(\n",
    "        emb[idx, 0],\n",
    "        emb[idx, 1],\n",
    "        c=color,\n",
    "        label=f'PI‑RADS {cls}',\n",
    "        alpha=0.7,\n",
    "        s=40\n",
    "    )\n",
    "\n",
    "plt.legend(title='PI‑RADS', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title('PCA(50) + t‑SNE Clustering of PI‑RADS Slices')\n",
    "plt.xlabel('t‑SNE Dim 1')\n",
    "plt.ylabel('t‑SNE Dim 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca8b58-4654-4051-abf1-82432d7a0fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
